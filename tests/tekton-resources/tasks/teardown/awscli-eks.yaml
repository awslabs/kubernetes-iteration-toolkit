---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: awscli-eks-cluster-teardown
  namespace: scalability
spec:
  description: |
    Teardown an EKS cluster.
    This Task can be used to teardown an EKS cluster with mng in an AWS account.
  params:
  - name: cluster-name
    description: The name of the EKS cluster which will be teared down.
  - name: region
    default: us-west-2
    description: The region where the cluster is in.
  - name: endpoint
    default: ""
  - name: namespace-count
    description: The number of namespaces for EKS Pod Identity test.
    default: "0"
  - name: slack-hook
    default: ""
  - name: slack-message
    default: "Job is completed"
  - name: service-role-stack-name
  - name: node-role-stack-name
  - name: launch-template-stack-name
  steps:
  - name: delete-cluster
    image: alpine/k8s:1.23.7
    script: |
      set +e
      ENDPOINT_FLAG=""
      if [ -n "$(params.endpoint)" ]; then
        ENDPOINT_FLAG="--endpoint $(params.endpoint)"
      fi

      for i in `aws eks list-nodegroups --cluster-name $(params.cluster-name) $ENDPOINT_FLAG --region $(params.region)  | jq -r '.nodegroups[]'`;
      do
          aws eks delete-nodegroup --nodegroup-name $i --cluster-name $(params.cluster-name) $ENDPOINT_FLAG --region $(params.region);
          aws eks wait nodegroup-deleted --nodegroup-name $i --cluster-name $(params.cluster-name) $ENDPOINT_FLAG --region $(params.region);
      done;
      echo "Starting to delete cluster..."
      aws eks delete-cluster --name $(params.cluster-name) --region $(params.region) $ENDPOINT_FLAG
      echo "Waiting for cluster to be deleted..."
      aws eks wait cluster-deleted --name $(params.cluster-name) --region $(params.region) $ENDPOINT_FLAG
      echo "Cluster is deleted..."

      MANAGED_POLICY_ARN="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
      for i in $(seq 1 $(params.namespace-count)); do
        PIA_ROLE_NAME=$(params.cluster-name)-pia-role-$i
        PIA_ROLE_EXISTS=$(aws iam get-role --role-name $PIA_ROLE_NAME --query 'Role.RoleName' --output text 2>/dev/null)
        if [ "$PIA_ROLE_EXISTS" == "$PIA_ROLE_NAME" ]; then
          aws iam detach-role-policy --role-name $PIA_ROLE_NAME --policy-arn $MANAGED_POLICY_ARN
          aws iam delete-role --role-name $PIA_ROLE_NAME
          echo "Role $PIA_ROLE_NAME deleted successfully."
        else
          echo "Role $PIA_ROLE_NAME does not exist, no action needed."
        fi
      done
  - name: awscli-delete-asg
    image: alpine/k8s:1.23.7
    script: |
      #!/bin/bash
      set -e
      aws sts get-caller-identity 
      # Stack ids for self managed node groups will have pattern <cluster-name>-nodes-<num>
      STACK_IDS=$(aws cloudformation describe-stacks \
        --region $(params.region) \
        --query 'Stacks[?contains(StackName, `'$(params.cluster-name)'-nodes-`)].StackName' \
        --output text)
      
      if [ -z "$STACK_IDS" ]; then
        echo "No stacks found matching pattern: $(params.cluster-name)-nodes-"
        exit 0
      fi
      
      echo "Found stacks to delete: $STACK_IDS"
      # Delete each stack and wait for completion
      for stack_name in $STACK_IDS; do
        echo "Deleting stack: $stack_name"
        
        # Delete the stack
        aws cloudformation delete-stack \
          --region $(params.region) \
          --stack-name "$stack_name"
        
        echo "Waiting for stack deletion to complete..."
        
        # Wait for deletion to complete
        aws cloudformation wait stack-delete-complete \
          --region $(params.region) \
          --stack-name "$stack_name"
        
        echo "Stack $stack_name deleted successfully!"
      done
      
      echo "All matching stacks have been deleted!"
  - name: teardown-eks-role-stack
    image: alpine/k8s:1.30.2
    script: |
      aws cloudformation delete-stack --stack-name $(params.service-role-stack-name)
      aws cloudformation delete-stack --stack-name $(params.launch-template-stack-name)
      # wait for the launch-template stack to be completely deleted to avoid race-conditions.
      echo "waiting for launch-template stack deletion..."
      aws cloudformation wait stack-delete-complete --stack-name $(params.launch-template-stack-name)
      STACK_STATUS=$(aws cloudformation describe-stacks --stack-name $(params.node-role-stack-name) --query 'Stacks[0].StackStatus' --output text || echo "STACK_NOT_FOUND")
      echo $STACK_STATUS
      if [ "$STACK_STATUS" == "DELETE_FAILED" ]; then
          echo "Stack is in DELETE_FAILED state, using FORCE_DELETE_STACK"
          aws cloudformation delete-stack --stack-name $(params.node-role-stack-name) --deletion-mode FORCE_DELETE_STACK
      else
          echo "Normal stack deletion"
          aws cloudformation delete-stack --stack-name $(params.node-role-stack-name)
      fi
      
  - name: awscli-delete-vpc
    image: alpine/k8s:1.23.7
    script: |
      #!/bin/bash
      aws sts get-caller-identity
      # Check if the stack exists
      aws cloudformation --region $(params.region) describe-stacks --stack-name $(params.cluster-name)
      if [ $? -ne 0 ]; then
        echo "Stack $(params.cluster-name) not found. Exiting..."
        exit 1
      else
        echo "Deleting stack $(params.cluster-name)..."
      fi
      #Deletes the CFN stack
      aws cloudformation delete-stack --region $(params.region) --stack-name $(params.cluster-name)
      # Wait for the stack to be deleted
      aws cloudformation wait stack-delete-complete --region $(params.region) --stack-name $(params.cluster-name)
      if [ $? -ne 0 ]; then
        # Get vpc id from stack that is blocking deletion
        VPC_ID=$(aws cloudformation --region $(params.region) describe-stacks --stack-name $(params.cluster-name) | jq -r '.Stacks[].Outputs[] | select(.OutputKey == "VpcId") | .OutputValue')
        echo "$VPC_ID"
        # Get security group ids from vpc that is blocking deletion. Have to get the security group this way because it doesn't report as part
        # of the stack. Filter only security groups with cluster-name key to prevent deleting default sg
        SG_IDS=$(aws ec2 describe-security-groups --region $(params.region) --filters 'Name=vpc-id, Values='$VPC_ID | jq -r '.SecurityGroups[] | select(. | select(.Tags != null) | .Tags[] | select(.Key == "aws:eks:cluster-name")) | .GroupId')
        echo "$SG_IDS"
        for sg in $SG_IDS; do
          echo "deleting security group $sg"
          aws ec2 delete-security-group --region $(params.region) --group-id "$sg"
          if [ $? -ne 0 ]; then
            echo "failed to delete security group $sg"
            exit 1
          fi
        done
        # Retry deleting the stack now that no resources should be blocking vpc deletion
        echo "retrying deleting stack $(params.cluster-name)..."
        #Deletes the CFN stack
        aws cloudformation delete-stack --region $(params.region) --stack-name $(params.cluster-name)
        # Wait for the stack to be deleted
        aws cloudformation wait stack-delete-complete --region $(params.region) --stack-name $(params.cluster-name)
        # Validate retry succeeded if not, error out
        if [ $? -ne 0 ]; then
          echo "retry failed to delete stack. Manual intervention required to cleanup resources"
          exit 1
        else
          echo "retry deleted successfully"
        fi
      else
        echo "Stack deleted successfully!"
      fi
  - name: send-slack-notification
    image: alpine/k8s:1.23.7
    script: |
      if [ -n "$(params.slack-hook)" ]; then
        curl -H "Content-type: application/json" --data '{"Message": "$(params.slack-message)"}' -X POST  $(params.slack-hook)
      fi